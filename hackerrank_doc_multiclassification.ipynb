{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackerrank_doc_multiclassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXXd/BFDvm8cZByfSAKHJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srichallla/Analyzing_Hotels_Dataset/blob/master/hackerrank_doc_multiclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n02DXQMnr78E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "94073d69-074f-47a2-b223-2b197981d9b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11p2LYWMzifB"
      },
      "source": [
        "dealing with class Imbalance problem. using both oversampling followed by undersampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdF_BxTrsg1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "314ad8f3-f8e3-4abc-e487-2d58b610d032"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#pkgs to deal with class imbalances\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "\n",
        "#The first line in input file contains \"5485\" adding \"skiprows\" skips 1st line\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/hackerrank_doc_classification.txt', header=None, skiprows=1)\n",
        "print(df.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                   0\n",
            "0  1 champion products ch approves stock split ch...\n",
            "1  2 computer terminal systems cpml completes sal...\n",
            "2  1 cobanco inc cbco year net shr cts vs dlrs ne...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sslGh7cyW76N"
      },
      "source": [
        "1) Read text data from CSV file.\n",
        "\n",
        "2) Check for missing values and impute them if needed.\n",
        "\n",
        "3) Check for duplicates and drop them if present.\n",
        "\n",
        "4) Verify if class Imbalance exists.\n",
        "\n",
        "5) Pre-process the data i.e convert to lowercase, remove special characters, digits, expand contarction, stemming, lemmetization, remove stop words etc...\n",
        "\n",
        "6) Tokenize sentence i.e. break sentences into words.\n",
        "\n",
        "7) Convert each tokenized sentence to a numerical vectors. Result will be a matrix of numbers, where each row corresponds to one sentence.\n",
        "\n",
        "8) Deal with class imbalances may be by using oversampling followed by undersampling or other techniques.\n",
        "\n",
        "9) Split the numerical matrix to Train and Test sets.\n",
        "\n",
        "10) Train the model.\n",
        "\n",
        "11) Use the trained model to classify unseen documents.\n",
        "\n",
        "12) Evaluate the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miVCBVN_xwax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "76ed9fe8-89be-44d0-f3fa-ace52e88bf66"
      },
      "source": [
        "sentences=[]\n",
        "labels=[]\n",
        "#Iterate through all rows 0th column and extract lables and sentences\n",
        "#If maxsplit is specified, the list will have the maximum of maxsplit+1 items, here each line is split into 2 items at 0->label at 1->whole sentence.\n",
        "for line in df.iloc[:,0]:\n",
        "    line=line.split(\" \", maxsplit=1)\n",
        "    labels.append(line[0])\n",
        "    sentences.append(line[1])\n",
        "\n",
        "sentences=pd.DataFrame(sentences)\n",
        "labels=pd.DataFrame(labels)\n",
        "df2=pd.concat([sentences,labels],axis=1) # Here column names will be '0', so giving names to columns \n",
        "df2.columns=[\"Sentence\",\"Label\"]\n",
        "\n",
        "#shuffling the order of the DataFrame's rows, so that all target labels are mixed\n",
        "#frac=1 means return all rows (in random order).\n",
        "#df2 = df2.sample(frac=1).reset_index(drop=True)\n",
        "print(df2.head(3))\n",
        "print(\"\\n Count of missing values in each column: \\n\", df2.isnull().sum()) # No missing values\n",
        "df2.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            Sentence Label\n",
            "0  champion products ch approves stock split cham...     1\n",
            "1  computer terminal systems cpml completes sale ...     2\n",
            "2  cobanco inc cbco year net shr cts vs dlrs net ...     1\n",
            "\n",
            " Count of missing values in each column: \n",
            " Sentence    0\n",
            "Label       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5485</td>\n",
              "      <td>5485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>5423</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>sumita says bank will intervene if necessary b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>3</td>\n",
              "      <td>2840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Sentence Label\n",
              "count                                                5485  5485\n",
              "unique                                               5423     8\n",
              "top     sumita says bank will intervene if necessary b...     1\n",
              "freq                                                    3  2840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7DclcnaH_iC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "97866816-8b3c-423a-84e4-320b30550932"
      },
      "source": [
        "#Handling duplicate values in the dataframe:\n",
        "print(\"Number of duplicates: \", sum(df2.duplicated()))\n",
        "#To see which rows are duplicated:\n",
        "df2.loc[df2.duplicated(keep=False),]\n",
        "#Drop duplicates except for first one\n",
        "df3=df2.drop_duplicates(keep='first')\n",
        "print(\"Number of duplicates after dropping duplicates: \", sum(df3.duplicated()))\n",
        "print(\"Dimenstions after removing duplicates: \",df3.shape)\n",
        "#https://github.com/irfanelahi-ds/document-classification-python/blob/master/document_classification_python_sklearn_nltk.ipynb\n",
        "print(\"Before: Class compositions percentage: \\n\",df3[\"Label\"].value_counts(normalize=True))\n",
        "print(\"Before: Class labes distribution counts: \\n\",df3[\"Label\"].value_counts())\n",
        "#Looks like we need to deal with class imbalances. Undersampling the majority classe(s) 1 to 400 & 2 to 400, also I will use over-sampling\n",
        "#minority classes using SMOTE 5:100. These techniques wont work on text data until they are transformed to numbers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of duplicates:  58\n",
            "Number of duplicates after dropping duplicates:  0\n",
            "Dimenstions after removing duplicates:  (5427, 2)\n",
            "Before: Class compositions percentage: \n",
            " 1    0.522388\n",
            "2    0.293164\n",
            "6    0.045145\n",
            "3    0.044039\n",
            "8    0.035931\n",
            "7    0.032430\n",
            "4    0.019348\n",
            "5    0.007555\n",
            "Name: Label, dtype: float64\n",
            "Before: Class labes distribution counts: \n",
            " 1    2835\n",
            "2    1591\n",
            "6     245\n",
            "3     239\n",
            "8     195\n",
            "7     176\n",
            "4     105\n",
            "5      41\n",
            "Name: Label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiW0QfaEyxi1"
      },
      "source": [
        "**Data Pre-processing and Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMkH1CWIy80T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "caf4c3ad-2294-4303-8f56-05e1d5164cd2"
      },
      "source": [
        "#Download stop words\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNDPEZMG-JvI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6225ead9-ba61-45ce-f545-87f6f4c6d7f6"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "print(len(STOP_WORDS))\n",
        "#Add custom stopwords to existing list, which are more common in traning data\n",
        "STOP_WORDS.add(\"reuter\")\n",
        "print(\"after : \" ,len(STOP_WORDS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326\n",
            "after :  327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltdQR-AT9lV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "bf0807e0-dfbd-45fd-f294-2dd1f61c58c3"
      },
      "source": [
        "\n",
        "corpus=df3['Sentence'] #Conatins 5427 sentences\n",
        "# Data Pre-processing. Converting to lower case, Remove special characters, tokenize(split sentence into words) and remove stop words\n",
        "def clean_corpus(corpus):\n",
        "    corpus_cleaned = []\n",
        "    # normalize each Sentence/line in the corpus\n",
        "    for sentence in corpus:\n",
        "        #Covert to lower case\n",
        "        sentence = sentence.lower()        \n",
        "        #Remove special characters\n",
        "        regex_pattern = re.compile(r'[\\,+\\:\\?\\!\\\"\\(\\)!\\'\\.\\%\\[\\]]+')\n",
        "        sentence = regex_pattern.sub(r' ', sentence)        \n",
        "        sentence = re.sub('[^a-zA-z\\s]', '', sentence)\n",
        "        #tokenize Sentences\n",
        "        tokenizer = ToktokTokenizer()\n",
        "        tokens = tokenizer.tokenize(sentence)        \n",
        "        #Remove stop words\n",
        "        tokens = [token.strip() for token in tokens]        \n",
        "        filtered_tokens = [token for token in tokens if token not in STOP_WORDS]  #Comma separated List of strings of one sentence  ['champion', 'products', 'ch',..]    \n",
        "        filtered_text = ' '.join(filtered_tokens) # line o/p's: champion products ch .... \n",
        "        #Truncate sentense to max 500 characters\n",
        "        filtered_text = filtered_text[0:499]       \n",
        "        corpus_cleaned.append(filtered_text)   \n",
        "    return  corpus_cleaned\n",
        "full_corpus = clean_corpus(corpus)\n",
        "#clean_corpus(corpus)\n",
        "print(\"We have %d lines/Sentences in the corpus.\" %len(full_corpus))\n",
        "full_corpus[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 5427 lines/Sentences in the corpus.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['champion products ch approves stock split champion products inc said board directors approved stock split common shares shareholders record april company said board voted recommend shareholders annual meeting april increase authorized capital stock mln mln shares',\n",
              " 'computer terminal systems cpml completes sale computer terminal systems inc said completed sale shares common stock warrants acquire additional mln shares sedio n v lugano switzerland dlrs company said warrants exercisable years purchase price dlrs share computer terminal said sedio right buy additional shares increase total holdings pct computer terminal s outstanding common stock certain circumstances involving change control company company said conditions occur warrants exercisable price eq',\n",
              " 'cobanco inc cbco year net shr cts vs dlrs net vs assets mln vs mln deposits mln vs mln loans mln vs mln note th qtr available year includes extraordinary gain tax carry forward dlrs cts shr',\n",
              " 'international inc nd qtr jan oper shr loss cts vs profit seven cts oper shr profit vs profit revs mln vs mln avg shrs mln vs mln mths oper shr profit nil vs profit cts oper net profit vs profit revs mln vs mln avg shrs mln vs mln note shr calculated payment preferred dividends results exclude credits cts cts qtr mths vs cts cts prior periods operating loss carryforwards',\n",
              " 'brown forman inc bfd th qtr net shr dlr vs cts net mln vs mln revs mln vs mln mths shr dlrs vs dlrs net mln vs mln revs billion vs mln',\n",
              " 'dean foods df sees strong th qtr earnings dean foods co expects earnings fourth quarter ending exceed year ago period chairman kenneth douglas told analysts fiscal fourth quarter food processor reported earnings cts share douglas said year s sales exceed billion dlrs billion dlrs prior year repeated earlier projection quarter earnings probably slightly year s cts share falling range cts cts share douglas said early project anticipated fourth quarter performance exceed prior year s overall earni',\n",
              " 'brown forman bfdb sets stock split ups payout brown forman inc said board approved stock split pct increase company cash dividend company cited improved earnings outlook continued strong cash flow reasons raising dividend brown forman said split class class b common shares effective march company said directors declared quarterly cash dividend new share classes cts payable april holders record march prior split company paid cts quarterly brown forman today reported pct increase quarter profits ',\n",
              " 'esquire radio electronics inc ee th qtr shr profit cts vs profit cts annual div cts vs cts prior yr net profit vs profit revs vs mths shr profit cts vs loss cts net profit vs loss revs mln vs note annual dividend payable april stockholders record march',\n",
              " 'united presidential corp upco th qtr net shr cts vs cts net vs revs mln vs mln year shr dlrs vs dlrs net vs revs mln vs mln note results include adjustment dlrs cts shr year periods improvement results universal life business estimated',\n",
              " 'owens minor inc obod raises qtly dividend qtly div eights cts vs cts prior pay march record march']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYvxdW2KCdDp"
      },
      "source": [
        "**Convert text to matrix of numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMNLM5uThvno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b495ee24-375b-40c6-fc71-5c985f3f6af6"
      },
      "source": [
        "# Initializing TFIDF vectorizer to convert the raw corpus to a matrix of TFIDF features.\n",
        "vectorizer = TfidfVectorizer(min_df=1,  # min count for relevant vocabulary\n",
        "    max_features=12000,  # maximum number of features\n",
        "    strip_accents='unicode',  # replace all accented unicode char \n",
        "    # by their corresponding  ASCII char\n",
        "    analyzer='word',  # features made of words\n",
        "    token_pattern=r'\\w{4,}',  # tokenize only words of 4+ chars\n",
        "    ngram_range=(1, 1),  # features made of a single tokens\n",
        "    #use_idf=True,  # enable inverse-document-frequency reweighting\n",
        "    #smooth_idf=True  # prevents zero division for unseen words\n",
        "    )\n",
        "# Creating TFIDF features sparse matrix by fitting it on the specified corpus.\n",
        "#tfidf_matrix=vectorizer.fit_transform(full_corpus).todense()\n",
        "tfidf_matrix=vectorizer.fit_transform(full_corpus)\n",
        "# Grabbing the name of the features.\n",
        "tfidf_names=vectorizer.get_feature_names()\n",
        "print(\"Number of TFIDF Features: %d\"%len(tfidf_names))\n",
        "print(\"Matrix shape = \",tfidf_matrix.shape)\n",
        "print(\"Feature Names:\")\n",
        "print(tfidf_names[:100])\n",
        "# There are 19657 columns that will be used for training the classifier\n",
        "#deal with class imbalances using oversampling followed by undersampling\n",
        "#smote_enn = SMOTEENN(random_state=0)\n",
        "smt = SMOTETomek(ratio='auto',random_state=0)\n",
        "X_balanced, y_balanced = smt.fit_resample(tfidf_matrix, df3[\"Label\"])\n",
        "X_balanced = X_balanced.todense()\n",
        "#splitting the data into random training and test sets for both independent variables and labels.\n",
        "X_train_corpus, X_test_corpus, y_labels_train, y_labels_test = train_test_split(X_balanced, y_balanced, test_size=.2)\n",
        "#analyzing the shape of the training and test data-set:\n",
        "print('Shape of Training Data: '+str(X_train_corpus.shape))\n",
        "print('Shape of Test Data: '+str(X_test_corpus.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of TFIDF Features: 12000\n",
            "Matrix shape =  (5427, 12000)\n",
            "Feature Names:\n",
            "['aachener', 'aagiy', 'aaix', 'aancor', 'aare', 'aarn', 'aarnoud', 'aaron', 'aati', 'abal', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abatement', 'abbas', 'abbett', 'abbey', 'abbreviated', 'abdel', 'abdelaziz', 'abdul', 'abdulaziz', 'abdullah', 'abev', 'abex', 'abide', 'abiding', 'abidjan', 'abilene', 'abilities', 'ability', 'ablaze', 'able', 'abnn', 'abnormally', 'aboard', 'abolish', 'abolished', 'abolishing', 'aborted', 'abover', 'abroad', 'abrupt', 'absb', 'absence', 'absent', 'absolute', 'absolutely', 'absolve', 'absorb', 'abundant', 'academics', 'academy', 'acccounts', 'accelerate', 'accelerated', 'accelerating', 'acceleration', 'accentuating', 'accept', 'acceptable', 'acceptance', 'acceptances', 'accepted', 'accepting', 'accepts', 'access', 'accession', 'accessories', 'accident', 'accidents', 'accommodate', 'accompanied', 'accompany', 'accompanying', 'accomplishments', 'accord', 'accordance', 'according', 'accordingly', 'accords', 'accou', 'accoun', 'account', 'accountants', 'accounted', 'accounting', 'accounts', 'accruable', 'accrual', 'accruals', 'accrue', 'accrued', 'accruing', 'accu', 'accumulated', 'accumulating', 'accuracy', 'accuray']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of Training Data: (18099, 12000)\n",
            "Shape of Test Data: (4525, 12000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7ACMqiNn94p"
      },
      "source": [
        "**Training The Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLTxAGIlMDk"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mn_bayes=MultinomialNB()\n",
        "mn_bayes_fit=mn_bayes.fit(X_train_corpus,y_labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IliM9GplTiZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "23e21fc9-d12b-40de-af10-0b86ddc3d208"
      },
      "source": [
        "\"\"\"test_corpus =[\"8 south korean won fixed at month high the bank of korea said it fixed the midrate of the won at to the dollar its highest level since february when it was the won was set at yesterday the won has risen pct against the dollar so far this year after rising pct in reuter\",\n",
        " \"6 occidental oxy in big peru heavy oil find occidental petroleum corp said the shiviyacu development well on block ab in the peruvian amazon jungle is producing barrels of degree gravity oil per day from depths of to and to feet the company said it is drilling a new exploration well on the block southeast shiviyacu miles away reuter \",\n",
        " \"7 tcf banking and savings hikes prime rate tcf banking and savings fa said it is raising its prime rate to pct from pct effective today \",\n",
        " \"7 royal bank canada ups u s base rate pct to pct \",\n",
        " \"5 cereals mcas to be unchanged next week monetary compensatory amounts mca s will be unchanged for the week starting april ec commission officials said cereals mca s are plus points for west germany and the netherlands minus two points for denmark minus eight points for france minus nine for ireland minus for italy minus for britain minus for greece and minus for spain reuter \",\n",
        " \"5 report on export markets for u s grain delayed the u s agriculture department s report on export markets for u s grain scheudled for release today has been delayed until wednesday april a department spokeswoman said no reason was given for the delay in releasing the monthly report reuter \",\n",
        " \"4 b and c reorganises commercial operations british and commonwealth shipping co plc bcom l said that it would reorganise its commercial and service operations into a single public grouping with autonomous management the group has expanded rapidly in the past year through the mln stg acquisition of exco international plc and mln bid for steel brothers holdings plc it noted that its operations were now divided between financial services including money broking investment management and forfaiting and more traditional areas such as aviation hotels commodity trading and office equipment it said that each sector had exciting prospects but required different methods of management and financing b and c planned to form a new public company to hold the commercial operations and envisaged it operating with a capital of between mln and mln stg it has retained barclays de zoete wedd to advise on the introduction of independent investors to subscribe for additional capital and believes that the proportion of equity capital held by outside investors would not exceed pct of the total the statement said that with the continued support of b and c together with outside capital the new grouping would emerge as a major group in its own right with the ability to take advantages of opportunities as they arose however the group would not seek a listing for the time being b and c also said that its chairman lord cayzer planned to retire in june the company proposed that he be appointed life president and that current chief executive john gunn should take over as chairman b and c shares eased p to p at gmt reuter \",\n",
        " \"4 biffex looking to join new futures exchange the baltic international freight futures exchange biffex said it agreed to pursue negotiations with other futures markets on the baltic exchange with a view to merging into a new futures exchange legal advisers have already been instructed to implement amalgamation of the london potato futures association the soya bean meal futures exchange and the london meat futures exchange the london grain futures market has also discussed merging with the other markets the aim of the merger is to seek recognised investment exchange status as required by the financial services act reuter\",\n",
        " \"6 next few months crucial for oil hernandez energy and mines minister arturo hernandez grisanti today told a meeting of regional oil exporters the next few months will be critical to efforts to achieve price recovery and stabilize the market hernandez said while opec and non opec nations have already made some strides in their efforts to strengthen the market the danger of a reversal is always present march and the next two or three months will be a really critical period hernandez said he said we will be able to define a movement either towards market stability and price recovery or depending on the market a reversal earlier this week hernandez said venezuela s oil price has averaged just above dlrs a barrel for the year to date if opec achieves its stated goal of an dlrs a barrel average price he said venezuela s should move up to dlrs hernandez spoke today at the opening of the fifth ministerial meeting of the informal group of latin american and caribbean oil exporters formed in ministers from member states ecuador mexico trinidad tobago and venezuela are attending the two day conference while colombia is present for the first time as an observer hernandez defined the meeting as an informal exchange of ideas about the oil market however the members will also discuss ways to combat proposals for a tax on imported oil currently before the u s congress following the opening session the group of ministers met with president jaime lusinchi at miraflores the presidential palace the delegations to the conference are headed by hernandez of venezuela energy minister javier espinosa of ecuador energy minister kelvin ramnath of trinidad tobago jose luis alcudiai assistant energy secretary of mexico and energy minister guilermno perry rubio of colombia reuter \",\n",
        " \"1 This is a document \",\n",
        " \"1 this is another document \",\n",
        " \"8 documents are seperated by newlines \"]\"\"\"\n",
        "\"\"\"clean_test_corpus = clean_corpus(X_test_corpus)\n",
        "print(\"Cleaned Test corpus:\\n\" ,clean_test_corpus)\n",
        "#Transforming sentence from words to numbers\n",
        "tv_test_features = vectorizer.transform(clean_test_corpus)\n",
        "print(\"Test features shape: \",tv_test_features.shape)\"\"\"\n",
        "\n",
        "prediction_mn=mn_bayes_fit.predict(X_test_corpus)\n",
        "print(\"Predicted Document Lables are : \\n\",prediction_mn) #Should output 8, but o/p is 1\n",
        "\"\"\"print(\"\\nCount of non-zeros elements in tfidf_matrix: \",np.count_nonzero(tfidf_matrix))\n",
        "#The corresponding non-zero values in the array can be obtained with\n",
        "print(\"tfidf_matrix dimenstions: \", tfidf_matrix.ndim)\n",
        "sizeoftext = np.array([len(e) for e in train_corpus])\n",
        "print(\"min length (i.e num of characters including spaces) of sentence in train corpus = %d max length of sentence = %d \"%(sizeoftext.min(),sizeoftext.max()))\n",
        "#There are 610 sentences whose size is above 1000 and 157 sentences whose size is above 2000\n",
        "print(\"tfidf_matrix[1] index dimenstions: \", tfidf_matrix[1].shape)\n",
        "print(\"non-zero elments in tfidf_matrix[1]: \")\n",
        "tfidf_matrix[np.nonzero(tfidf_matrix[1])]\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Document Lables are : \n",
            " ['4' '5' '1' ... '2' '2' '2']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(\"\\nCount of non-zeros elements in tfidf_matrix: \",np.count_nonzero(tfidf_matrix))\\n#The corresponding non-zero values in the array can be obtained with\\nprint(\"tfidf_matrix dimenstions: \", tfidf_matrix.ndim)\\nsizeoftext = np.array([len(e) for e in train_corpus])\\nprint(\"min length (i.e num of characters including spaces) of sentence in train corpus = %d max length of sentence = %d \"%(sizeoftext.min(),sizeoftext.max()))\\n#There are 610 sentences whose size is above 1000 and 157 sentences whose size is above 2000\\nprint(\"tfidf_matrix[1] index dimenstions: \", tfidf_matrix[1].shape)\\nprint(\"non-zero elments in tfidf_matrix[1]: \")\\ntfidf_matrix[np.nonzero(tfidf_matrix[1])]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaCg7HcIMXdA"
      },
      "source": [
        "with 500 sentence lenght it gives 16492 features.\n",
        "with 300 sentence lenght it gives 14792 features.\n",
        "with no sentence truvcation it gives 19,000 features\n",
        "\n",
        "**Others Promising Evenues to Pursue:**\n",
        "1) See if you can delete columns/features from \"vectorizer.get_feature_names()\" if feature name is not present in the corpus vocabulary Or cleaned text document\n",
        "2) Dealing with class imbalance, stratified smapling etc.\n",
        "3) Randomly Shuffle rows, to ensure same class/traget values are not together (Doing this did not imporve acccuracy)\n",
        "\n",
        "**To deal with Imabalanced datasets, I will be doing below:**\n",
        "1) Under-sampling majority calsse(s) \"1\" to 300 & \"2\" 250 using Cluster Centroids (from imblearn.under_sampling import ClusterCentroids)\n",
        "https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "2) Over-sampling Minority class 5 to 100 using SMOTE (from imblearn.over_sampling import SMOTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvAUQ7ySxoVe"
      },
      "source": [
        "**Performance metric**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ-AadU3wbSn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "552602c1-783a-48e6-ab92-0174e8f27156"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
        "#labels_test = ['8', '6', '7' ,'7', '5', '5', '4', '4', '6' ,'1', '1', '8']\n",
        "#Navie Bayes Accuracy Score\n",
        "nb_ascore = accuracy_score(y_labels_test, prediction_mn)\n",
        "print(\"Navie Bayes Accuracy Score: \", nb_ascore)\n",
        "print(\"Confusion Matrix of Bernoulli Naive Bayes Classifier output: \")\n",
        "confusion_matrix(y_labels_test,prediction_mn)\n",
        "print(\"Classification Metrics: \")\n",
        "print(classification_report(y_labels_test,prediction_mn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Navie Bayes Accuracy Score:  0.9858563535911602\n",
            "Confusion Matrix of Bernoulli Naive Bayes Classifier output: \n",
            "Classification Metrics: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.95      0.97       512\n",
            "           2       0.97      0.97      0.97       598\n",
            "           3       0.99      1.00      0.99       563\n",
            "           4       0.99      1.00      1.00       574\n",
            "           5       1.00      1.00      1.00       549\n",
            "           6       0.99      0.99      0.99       582\n",
            "           7       0.97      0.99      0.98       583\n",
            "           8       0.98      0.98      0.98       564\n",
            "\n",
            "    accuracy                           0.99      4525\n",
            "   macro avg       0.99      0.99      0.99      4525\n",
            "weighted avg       0.99      0.99      0.99      4525\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}